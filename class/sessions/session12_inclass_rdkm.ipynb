{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 12 - Measuring environmental impact\n",
    "\n",
    "In this session, we're going to look at one particular way that we can measure the impact of our code on the world around us. In particular, we're going to be looking at how we can approximate the *environmental impact* of our cultural data science footprint.\n",
    "\n",
    "To do this, we're going to use the open-source software package *CodeCarbon*. You can find more information at the following links:\n",
    "\n",
    "- CodeCarbon Website: [https://codecarbon.io/](https://codecarbon.io/)\n",
    "- GitHub Repo: [https://mlco2.github.io/codecarbon/](https://mlco2.github.io/codecarbon/)\n",
    "- Documentation: [https://mlco2.github.io/codecarbon/](https://mlco2.github.io/codecarbon/)\n",
    "\n",
    "We'll do some testing on HuggingFace pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing HuggingFace pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-25 10:34:53.554293: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-25 10:34:53.558316: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-25 10:34:53.608511: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-25 10:34:54.514927: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from codecarbon import EmissionsTracker\n",
    "from transformers import pipeline\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Text summarization pipeline__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may remember from a couple of weeks ago that *text summarization* was quite a compute intensive task. So let's see exactly how compute intensive it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. \n",
    "For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. \n",
    "On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. \n",
    "In the former task our best model outperforms even all previously reported ensembles.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google-t5/t5-small and revision d769bba (https://huggingface.co/google-t5/t5-small).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72785039f46a4709be36e9c11118b56e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d183522f3d9245ea93f0139932c7881e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d62eca137b4d4a812051263030faad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a6309b44b24411e8970a53be22f5f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46efe7062ed141bcacb12a446d99db89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summarizer = pipeline(task=\"summarization\", \n",
    "                      min_length=10,\n",
    "                      max_length=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of different ways that we can work with CodeCarbon, all of which is clearly explained in the relevant documentation.\n",
    "\n",
    "We'll go through each of them one at a time here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1 - Creating a tracker object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:35:19] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 10:35:19] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 10:35:19] No GPU found.\n",
      "[codecarbon INFO @ 10:35:19] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 10:35:19] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon WARNING @ 10:35:21] We saw that you have a Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 10:35:21] CPU Model on constant consumption mode: Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 10:35:21] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 10:35:21]   Platform system: Linux-5.4.256.el8-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 10:35:21]   Python version: 3.10.12\n",
      "[codecarbon INFO @ 10:35:21]   CodeCarbon version: 2.3.5\n",
      "[codecarbon INFO @ 10:35:21]   Available RAM : 376.535 GB\n",
      "[codecarbon INFO @ 10:35:21]   CPU count: 64\n",
      "[codecarbon INFO @ 10:35:21]   CPU model: Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 10:35:21]   GPU count: None\n",
      "[codecarbon INFO @ 10:35:21]   GPU model: None\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1714034125.135901    6829 service.cc:145] XLA service 0x5566625b2980 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1714034125.135980    6829 service.cc:153]   StreamExecutor device (0): Host, Default Version\n",
      "2024-04-25 10:35:25.141177: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1714034125.180708    6829 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[codecarbon INFO @ 10:35:32] Energy consumed for RAM : 0.000329 kWh. RAM Power : 141.20075225830078 W\n",
      "[codecarbon INFO @ 10:35:32] Energy consumed for all CPUs : 0.000099 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:35:32] 0.000428 kWh of electricity used since the beginning.\n",
      "/home/ucloud/.local/lib/python3.10/site-packages/codecarbon/output.py:168: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame.from_records([dict(data.values)])])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.718634290360947e-05"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracker = EmissionsTracker()\n",
    "tracker.start()\n",
    "summary = summarizer(text)\n",
    "tracker.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2 - Context manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:35:53] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 10:35:53] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 10:35:53] No GPU found.\n",
      "[codecarbon INFO @ 10:35:53] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 10:35:53] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon WARNING @ 10:35:54] We saw that you have a Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 10:35:54] CPU Model on constant consumption mode: Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 10:35:54] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 10:35:54]   Platform system: Linux-5.4.256.el8-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 10:35:54]   Python version: 3.10.12\n",
      "[codecarbon INFO @ 10:35:54]   CodeCarbon version: 2.3.5\n",
      "[codecarbon INFO @ 10:35:54]   Available RAM : 376.535 GB\n",
      "[codecarbon INFO @ 10:35:54]   CPU count: 64\n",
      "[codecarbon INFO @ 10:35:54]   CPU model: Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 10:35:54]   GPU count: None\n",
      "[codecarbon INFO @ 10:35:54]   GPU model: None\n",
      "[codecarbon INFO @ 10:36:05] Energy consumed for RAM : 0.000279 kWh. RAM Power : 141.20075225830078 W\n",
      "[codecarbon INFO @ 10:36:05] Energy consumed for all CPUs : 0.000084 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:36:05] 0.000363 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'the Transformer replaces recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention .'}]\n"
     ]
    }
   ],
   "source": [
    "with EmissionsTracker() as tracker:\n",
    "    summary = summarizer(text)\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3 - A Python decoractor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from codecarbon import track_emissions\n",
    "\n",
    "@track_emissions\n",
    "def summarization(text):\n",
    "    summary = summarizer(text)\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:46:10] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 10:46:10] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 10:46:10] No GPU found.\n",
      "[codecarbon INFO @ 10:46:10] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 10:46:10] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon WARNING @ 10:46:11] We saw that you have a Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 10:46:11] CPU Model on constant consumption mode: Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 10:46:11] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 10:46:11]   Platform system: Linux-5.4.256.el8-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 10:46:11]   Python version: 3.10.12\n",
      "[codecarbon INFO @ 10:46:11]   CodeCarbon version: 2.3.5\n",
      "[codecarbon INFO @ 10:46:11]   Available RAM : 376.535 GB\n",
      "[codecarbon INFO @ 10:46:11]   CPU count: 64\n",
      "[codecarbon INFO @ 10:46:11]   CPU model: Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 10:46:11]   GPU count: None\n",
      "[codecarbon INFO @ 10:46:11]   GPU model: None\n",
      "[codecarbon INFO @ 10:46:22] \n",
      "Graceful stopping: collecting and writing information.\n",
      "Please wait a few seconds...\n",
      "[codecarbon INFO @ 10:46:22] Energy consumed for RAM : 0.000294 kWh. RAM Power : 141.20075225830078 W\n",
      "[codecarbon INFO @ 10:46:22] Energy consumed for all CPUs : 0.000088 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:46:22] 0.000382 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:46:22] Done!\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'the Transformer replaces recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention .'}]\n"
     ]
    }
   ],
   "source": [
    "summarization(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A more complex example\n",
    "\n",
    "We can make the results more useful by changing the tracker parameters - full list can be found here [https://mlco2.github.io/codecarbon/parameters.html](https://mlco2.github.io/codecarbon/parameters.html).\n",
    "\n",
    "In the example that follows, we're going to download a HuggingFace dataset and a pretrained emotion classification model. \n",
    "\n",
    "We also introduce specific *tasks* to more clearly understand the impact of different parts of our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:51:44] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 10:51:44] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 10:51:44] No GPU found.\n",
      "[codecarbon INFO @ 10:51:44] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 10:51:44] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon WARNING @ 10:51:46] We saw that you have a Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz but we don't know it. Please contact us.\n",
      "[codecarbon INFO @ 10:51:46] CPU Model on constant consumption mode: Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 10:51:46] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 10:51:46]   Platform system: Linux-5.4.256.el8-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 10:51:46]   Python version: 3.10.12\n",
      "[codecarbon INFO @ 10:51:46]   CodeCarbon version: 2.3.5\n",
      "[codecarbon INFO @ 10:51:46]   Available RAM : 376.535 GB\n",
      "[codecarbon INFO @ 10:51:46]   CPU count: 64\n",
      "[codecarbon INFO @ 10:51:46]   CPU model: Intel(R) Xeon(R) Gold 6130 CPU @ 2.10GHz\n",
      "[codecarbon INFO @ 10:51:46]   GPU count: None\n",
      "[codecarbon INFO @ 10:51:46]   GPU model: None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aa4290db87b45e2a4cd85c7502979a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e87cb3a200964a709976d42129260c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a98b17ae7c64a7b842e0f4573069289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa3e46b15794ceb8b2562f74bf1ee69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12b5738e6743421e92fa51a6bad788e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a46da0fa5fb245e5a8fe6e44f6f6c24d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "785f8ad025d343e2b8aaf1fb587478a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:52:01] Energy consumed for RAM : 0.000470 kWh. RAM Power : 141.20075225830078 W\n",
      "[codecarbon INFO @ 10:52:01] Energy consumed for all CPUs : 0.000142 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:52:01] 0.000612 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f337c5ab86ca4ef4a824a9ea5d80ed02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/768 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d08a9b6862d416989f3e7b15e484a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tf_model.h5:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-emotion.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f35230afd9f47d6bd8f6662526d357f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "358a3039fdf1410a89539a5d9678ef42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf458d779de4244846b17374edbbee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 10:52:10] Energy consumed for RAM : 0.000812 kWh. RAM Power : 141.20075225830078 W\n",
      "[codecarbon INFO @ 10:52:10] Energy consumed for all CPUs : 0.000244 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:52:10] 0.001056 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41cf13aab76f421db99a67f533e94498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 10:55:54] Background scheduler didn't run for a long period (224s), results might be inaccurate\n",
      "[codecarbon INFO @ 10:55:54] Energy consumed for RAM : 0.009614 kWh. RAM Power : 141.20075225830078 W\n",
      "[codecarbon INFO @ 10:55:54] Energy consumed for all CPUs : 0.002894 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:55:54] 0.012508 kWh of electricity used since the beginning.\n",
      "[codecarbon INFO @ 10:55:54] Energy consumed for RAM : 0.009614 kWh. RAM Power : 141.20075225830078 W\n",
      "[codecarbon INFO @ 10:55:54] Energy consumed for all CPUs : 0.002894 kWh. Total CPU Power : 42.5 W\n",
      "[codecarbon INFO @ 10:55:54] 0.012508 kWh of electricity used since the beginning.\n",
      "/home/ucloud/.local/lib/python3.10/site-packages/codecarbon/output.py:168: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame.from_records([dict(data.values)])])\n",
      "/home/ucloud/.local/lib/python3.10/site-packages/codecarbon/output.py:197: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0022566208286612438"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outfolder = os.path.join(\"..\", \"emissions\")\n",
    "os.mkdir(outfolder)\n",
    "\n",
    "tracker = EmissionsTracker(project_name=\"sentiment classification\",\n",
    "                           experiment_id=\"sentiment_classifier\",\n",
    "                           output_dir=outfolder,\n",
    "                           output_file=\"emissions_sentiment.csv\")\n",
    "\n",
    "# tracking data downloading\n",
    "tracker.start_task(\"load dataset\")\n",
    "dataset = datasets.load_dataset(\"imdb\", \n",
    "                                split=\"test\")\n",
    "imdb_emissions = tracker.stop_task()\n",
    "\n",
    "# tracking downloading and initializing model\n",
    "tracker.start_task(\"build model\")\n",
    "classifier = pipeline(task=\"sentiment-analysis\", \n",
    "                      model=\"cardiffnlp/twitter-roberta-base-emotion\")\n",
    "model_emissions = tracker.stop_task()\n",
    "\n",
    "# tracking classification pipeline\n",
    "tracker.start_task(\"run classification\")\n",
    "preds = []\n",
    "for row in tqdm(dataset[\"text\"][:1000]):\n",
    "    preds.append(classifier(row[:100]))\n",
    "classifier_emissions = tracker.stop_task()\n",
    "\n",
    "tracker.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Inspecting the results__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "read_csv() missing 1 required positional argument: 'filepath_or_buffer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/work/SabrinaSchrollZakiHansen#5217/LA/cds_la/class/sessions/session12_inclass_rdkm.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://app-5040508-0.cloud.sdu.dk/work/SabrinaSchrollZakiHansen%235217/LA/cds_la/class/sessions/session12_inclass_rdkm.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m emissions_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv()\n",
      "\u001b[0;31mTypeError\u001b[0m: read_csv() missing 1 required positional argument: 'filepath_or_buffer'"
     ]
    }
   ],
   "source": [
    "emissions_df = pd.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "- Now that you have the basics down, head over and consider Assignment 5!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
